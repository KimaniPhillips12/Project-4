{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import re\n",
    "# Performance metric\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import nltk\n",
    "â€‹\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "book_summary_df = pd.read_csv(\"../input/book-genre-prediction-data-preparation/book_summaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_genres = [#'Speculative fiction',\n",
    "                'Science Fiction','Crime Fiction','Non-fiction','Children\\'s literature',\n",
    "                'Fantasy', 'Mystery', 'Suspense', 'Young adult literature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=book_summary_df, order=book_summary_df.Genres.value_counts().index, y='Genres')\n",
    "plt.title(\"Counts per Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_summary_df['String Counts'] = book_summary_df.Genres.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "df = book_summary_df.groupby(['Genres'])['String Counts'].sum().reset_index()\n",
    "df.sort_values(['String Counts'], ascending=True).head(10).plot(kind='bar', y='String Counts', x='Genres',\n",
    "                legend=False, color=['tab:orange', 'tab:green', 'tab:blue', 'tab:brown', 'tab:pink', 'tab:purple','tab:red', 'tab:gray', 'tab:olive'], ax=plt.gca())\n",
    "plt.title(\"Total Words per Summary per Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "df = book_summary_df.groupby(['Genres'])['String Counts'].sum().reset_index()\n",
    "df.sort_values(['String Counts'], ascending=True).head(10).plot(kind='bar', y='String Counts', x='Genres',\n",
    "                legend=False, color=['tab:orange', 'tab:green', 'tab:blue', 'tab:brown', 'tab:pink', 'tab:purple','tab:red', 'tab:gray', 'tab:olive'], ax=plt.gca())\n",
    "plt.title(\"Total Words per Summary per Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_summary_df['Summary'] = book_summary_df['Summary'].map(lambda summary : clean(summary))\n",
    "book_summary_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training, Validation and Test\n",
    "#Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split to train and test data\n",
    "train, test = train_test_split(book_summary_df, random_state=42, test_size=0.2, shuffle=True, stratify=book_summary_df['Genres'])\n",
    "\n",
    "train_x = train.Summary\n",
    "train_y = train.Genres\n",
    "test_x = test.Summary.to_numpy()\n",
    "test_y = test.Genres.to_numpy()\n",
    "test_titles = test['Book Title'].to_numpy()\n",
    "\n",
    "print(\"Training dataset = {}\".format(len(train_x)))\n",
    "print(\"Testing dataset = {}\".format(len(test_x)))\n",
    "colors=['tab:orange', 'tab:green', 'tab:blue', 'tab:brown', 'tab:pink', 'tab:purple','tab:red', 'tab:gray', 'tab:olive']\n",
    "plt.figure(figsize=(25,8))\n",
    "plt.subplot(1,2,1).set_title(\"Train Dataset-Counts per Genre\")\n",
    "train.groupby('Genres').size().sort_values(ascending=True).plot(kind='barh', color=colors,ax=plt.gca())\n",
    "plt.subplot(1,2,2).set_title(\"Test Dataset-Counts per Genre\")\n",
    "test.groupby('Genres').size().sort_values(ascending=True).plot(kind='barh', color=colors,ax=plt.gca())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature Extraction\n",
    "#For all the models, TFIDF vectors have been used and he classifier used is the OneVsRestClassifier from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from time import time\n",
    "# [training duration, testing duration, Test Accuracy\n",
    "\n",
    "benchmarks = {'NB' : [0.0, 0.0, 0.0],\n",
    "              'NB_tuned':  [0.0, 0.0, 0.0],\n",
    "              'SVC' :  [0.0, 0.0, 0.0],\n",
    "              'SVC_tuned':  [0.0, 0.0, 0.0],\n",
    "              'LR' :  [0.0, 0.0, 0.0],\n",
    "              'LR_tuned':  [0.0, 0.0, 0.0],\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1), use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline combining a text feature extractor with multi class classifier\n",
    "t0 = time()\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1), use_idf=True)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB())),\n",
    "            ])\n",
    "NB_pipeline.fit(train_x, train_y)\n",
    "benchmarks['NB'][0] = (time() - t0)/60\n",
    "filename = \"./NB_model.sav\"\n",
    "joblib.dump(NB_pipeline, filename)\n",
    "print(\"Training took {:.3f} [seconds] to complete and has been saved as {}\".format(benchmarks['NB'][0],filename))\n",
    "print(\"####Before tuning:####\")\n",
    "print('Train Accuracy : %.3f'%NB_pipeline.score(train_x, train_y))\n",
    "print('Test Accuracy : %.3f'%NB_pipeline.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the model\n",
    "NB_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "parameters = {    \n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__lowercase': (True, False), \n",
    "    'tfidf__norm': ('l1', 'l2'),  \n",
    "    'clf__estimator__alpha': (1, 0.1, 0.01, 0.001, 0.0001)  \n",
    "     }\n",
    "NB_grid = GridSearchCV(NB_pipeline, param_grid=parameters, n_jobs=-1, verbose=5)\n",
    "NB_grid.fit(train_x, train_y)\n",
    "#print(\"Training took: {:.2f} \".format(time() - t0))\n",
    "benchmarks['NB_tuned'][0] = (time() - t0)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./NB_tuned_model.sav\"\n",
    "joblib.dump(NB_grid, filename)\n",
    "print('Best Parameters : ',NB_grid.best_params_)\n",
    "print(\"Training took: {:.3f}[minutes] to complete and has been saved as {}\".format(benchmarks['NB_tuned'][0]/60,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"####After tuning:####\")\n",
    "print('Train Accuracy : %.3f'%NB_grid.best_estimator_.score(train_x, train_y))\n",
    "print('Test Accuracy : %.3f'%NB_grid.best_estimator_.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: Support Vector Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words, ngram_range=(1,2))),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "SVC_pipeline.fit(train_x, train_y)\n",
    "benchmarks['SVC'][0] = (time() - t0)/60\n",
    "#print(\"Training complete! Saving trained model....\")\n",
    "filename = \"./SVC_model.sav\"\n",
    "joblib.dump(SVC_pipeline, filename)\n",
    "print(\"Training took: {:.3f}[seconds] to complete and has been saved as {}\".format(benchmarks['SVC'][0],filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"####Before tuning:####\")\n",
    "print('Train Accuracy : %.3f'%SVC_pipeline.score(train_x, train_y))\n",
    "print('Test Accuracy : %.3f'%SVC_pipeline.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning for SVC\n",
    "#SVC model doesn't need any tuning because the train and test accuracy score are already quite near to the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "parameters = {\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'tfidf__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "              'clf__estimator__loss' : ['hinge', 'squared_hinge'],\n",
    "              #'tfidf__ngram_range': [(1,1), (1,2),(1,3)],\n",
    "              'clf__estimator__penalty' : [\"l1\", \"l2\"],\n",
    "              'clf__estimator__fit_intercept': [True, False],\n",
    "              'clf__estimator__C': [0.01, 1.0, 2.0]\n",
    "              #'clf__estimator__solver': ('newton-cg', 'sag','saga','lbfgs')\n",
    "             }\n",
    "SVC_grid = GridSearchCV(SVC_pipeline,param_grid=parameters, n_jobs=-1, verbose=5)\n",
    "SVC_grid.fit(train_x, train_y)\n",
    "#print(\"Training took: {:.2f} \".format(time() - t0))\n",
    "benchmarks['SVC_tuned'][0] = (time() - t0)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./SVC_tuned_model.sav\"\n",
    "joblib.dump(SVC_grid, filename)\n",
    "print('Best Parameters : ',SVC_grid.best_params_)\n",
    "print(\"Training took: {:.3f}[minutes] to complete and has been saved as {}\".format(benchmarks['SVC_tuned'][0]/60,filename))\n",
    "print(\"####After tuning:####\")\n",
    "print('Train Accuracy : %.3f'%SVC_grid.best_estimator_.score(train_x, train_y))\n",
    "print('Test Accuracy : %.3f'%SVC_grid.best_estimator_.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3: Logistic Regression\n",
    "t0 = time()\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1))),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='liblinear'), n_jobs=1)),\n",
    "            ])\n",
    "LogReg_pipeline.fit(train_x, train_y)\n",
    "benchmarks['LR'][0] = (time() - t0)/60\n",
    "#print(\"Training complete! Saving trained model....\")\n",
    "filename = \"./LogReg_model.sav\"\n",
    "joblib.dump(LogReg_pipeline, filename)\n",
    "print(\"Training took: {:.3f}[seconds] to complete and has been saved as {}\".format(benchmarks['LR'][0],filename))\n",
    "print(\"####Before tuning:####\")\n",
    "print('Train Accuracy : %.3f'%LogReg_pipeline.score(train_x, train_y))\n",
    "print('Test Accuracy : %.3f'%LogReg_pipeline.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer(stop_words=stop_words, ngram_range=(1,3))\n",
    "t0 = time()\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words, ngram_range=(1,1))),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "LogReg_pipeline.fit(train_x, train_y)\n",
    "benchmarks['LR'][0] = (time() - t0)/60\n",
    "#print(\"Training complete! Saving trained model....\")\n",
    "filename = \"./LogReg_model.sav\"\n",
    "joblib.dump(LogReg_pipeline, filename)\n",
    "print(\"Training took: {:.3f}[seconds] to complete and has been saved as {}\".format(benchmarks['LR'][0],filename))\n",
    "print(\"####Before tuning:####\")\n",
    "print('Train Accuracy : %.3f'%LogReg_pipeline.score(train_x, train_y))\n",
    "print('Test Accuracy : %.3f'%LogReg_pipeline.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "parameters = {'clf__estimator__penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              #'tfidf__max_df': [0.3, 0.5, 0.8, 1.0],\n",
    "              'clf__estimator__fit_intercept': [True, False],\n",
    "              'clf__estimator__C': [0.01, 1.0, 2.0],\n",
    "              #'clf__estimator__max_iter': [25]\n",
    "              'clf__estimator__solver': ('newton-cg', 'sag','saga','lbfgs')\n",
    "             }\n",
    "LogReg_grid = GridSearchCV(LogReg_pipeline,param_grid=parameters, n_jobs=-1, verbose=5)\n",
    "LogReg_grid.fit(train_x, train_y)\n",
    "#print(\"Training took: {:.2f} \".format(time() - t0))\n",
    "benchmarks['LR_tuned'][0] = (time() - t0)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./LR_tuned_model.sav\"\n",
    "joblib.dump(LogReg_grid, filename)\n",
    "print('Best Parameters : ',LogReg_grid.best_params_)\n",
    "print(\"Training took: {:.3f}[minutes] to complete and has been saved as {}\".format(benchmarks['LR_tuned'][0]/60,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"####After tuning:####\")\n",
    "print('Train Accuracy : %.3f'%LogReg_grid.best_estimator_.score(train_x, train_y))\n",
    "print('Test Accuracy : %.3f'%LogReg_grid.best_estimator_.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Models\n",
    "def save_benchmarks(estimator, estimator_name,x, y, benchmarks):\n",
    "    t0 = time()\n",
    "    pred = estimator.predict(test_x)\n",
    "    benchmarks[estimator_name][1] = (time() - t0)/60\n",
    "    benchmarks[estimator_name][2] = accuracy_score(y, pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB\n",
    "save_benchmarks(estimator=NB_pipeline, estimator_name='NB', x=test_x, y=test_y, benchmarks=benchmarks)\n",
    "save_benchmarks(estimator=NB_grid.best_estimator_, estimator_name='NB_tuned', x=test_x, y=test_y, benchmarks=benchmarks)\n",
    "#LR\n",
    "save_benchmarks(estimator=LogReg_pipeline, estimator_name='LR', x=test_x, y=test_y, benchmarks=benchmarks)\n",
    "save_benchmarks(estimator=LogReg_grid.best_estimator_, estimator_name='LR_tuned', x=test_x, y=test_y, benchmarks=benchmarks)\n",
    "#SVC\n",
    "save_benchmarks(estimator=SVC_pipeline, estimator_name='SVC', x=test_x, y=test_y, benchmarks=benchmarks)\n",
    "save_benchmarks(estimator=SVC_grid.best_estimator_, estimator_name='SVC_tuned', x=test_x, y=test_y, benchmarks=benchmarks)\n",
    "print(\"Benchmaks created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(benchmarks, orient='index',columns=['Trained Duration[m]', 'Testing Duration[m]', 'Accuracy(%)'])\n",
    "df.index.rename('Models', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_print_results(pred, labels, titles, save_file):\n",
    "    print('##########################################\\n#\\tTest accuracy is {:.4f}%\\t#\\n##########################################'.format(accuracy_score(test_y, \n",
    "    pred)*100))\n",
    "    pred_dict = {'titles':[],\n",
    "                 'genres': [],\n",
    "                 'prediction': [],\n",
    "                'result':[]\n",
    "                }\n",
    "    for i in range(len(labels)):\n",
    "        if (labels[i] == pred[i]):\n",
    "            prediction = 'Correct'\n",
    "        else:\n",
    "            prediction = 'Wrong'\n",
    "        pred_dict['titles'].append(titles[i])\n",
    "        pred_dict['genres'].append(labels[i])\n",
    "        pred_dict['prediction'].append(pred[i])\n",
    "        pred_dict['result'].append(prediction)\n",
    "    pred_df = pd.DataFrame.from_dict(pred_dict)\n",
    "    #Save to csv file\n",
    "    pred_df.to_csv(save_file)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nb = NB_grid.best_estimator_.predict(test_x)\n",
    "pred_nb_df = save_print_results(pred=pred_nb, labels=test_y, titles=test_titles, save_file=\"./pred_nb_results.csv\")\n",
    "pred_nb_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = LogReg_grid.best_estimator_.predict(test_x)\n",
    "pred_lr_df = save_print_results(pred=pred_lr, labels=test_y, titles=test_titles, save_file=\"./pred_lr_results.csv\")\n",
    "pred_lr_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svc = SVC_grid.best_estimator_.predict(test_x)\n",
    "pred_svc_df = save_print_results(pred_svc, test_y, test_titles, save_file=\"./pred_svc_results.csv\")\n",
    "pred_svc_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['tab:orange', 'tab:green', 'tab:blue', 'tab:brown', 'tab:pink', 'tab:purple','tab:red', 'tab:gray', 'tab:olive']\n",
    "#df = book_summary_df.groupby(['Genres'])['String Counts'].sum().reset_index()\n",
    "#Stats\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2,2,1).set_title(\"Prediction with LR Accuracy = {:.4f}%\".format(benchmarks['LR_tuned'][2]))\n",
    "pred_lr_df.pivot_table(index='genres',columns=['result'],aggfunc='size').plot(kind='barh', color=['tab:blue', 'tab:orange'],ax=plt.gca())\n",
    "plt.subplot(2,2,2).set_title(\"Prediction with SVC Accuracy = {:.4f}%\".format(benchmarks['SVC_tuned'][2]))\n",
    "pred_svc_df.pivot_table(index='genres',columns=['result'],aggfunc='size').plot(kind='barh', color=['tab:blue', 'tab:orange'],ax=plt.gca())\n",
    "plt.subplot(2,2,3).set_title(\"Prediction with NB  Accuracy = {:.4f}%\".format(benchmarks['NB_tuned'][2]))\n",
    "pred_nb_df.pivot_table(index='genres',columns=['result'],aggfunc='size').plot(kind='barh', color=['tab:blue', 'tab:orange'],ax=plt.gca())\n",
    "plt.subplot(2,2,4).set_title(\"Counts of Summaries per Genre\")\n",
    "book_summary_df.groupby('Genres').size().sort_values(ascending=True).plot(kind='barh', color=colors,ax=plt.gca())\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def classification_report(y, pred, target, name):\n",
    "    print('##########################################\\n#\\tTest accuracy is {:.4f}%\\t#\\n##########################################'.format(accuracy_score(y, \n",
    "    pred)*100))\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"Classification Report for model {}\".format(name))\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(metrics.classification_report(y, pred, target_names=target, zero_division=0))\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    #print(\"Confusion Matrix for {}\".format(name))\n",
    "    #print(\"------------------------------------------------------------\")\n",
    "    #print(metrics.confusion_matrix(y, pred))\n",
    "    plt.figure(figsize = (20,15))\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(metrics.confusion_matrix(y, pred), xticklabels = target, yticklabels = target, annot = True, fmt=\"d\",cmap = 'summer', annot_kws={\"size\": 12})\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(\"Confusion Matrix for {}\".format(name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(test_y, pred_nb, valid_genres, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(test_y, pred_svc, valid_genres, \"Support Vector Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(test_y, pred_lr, valid_genres, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference Function\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def clean(summary):\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    text = summary.translate(table)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\",\" \", summary.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre(plot):\n",
    "    s = clean(plot)\n",
    "    lr_est = joblib.load('./LR_tuned_model.sav')\n",
    "    svc_est = joblib.load('./SVC_tuned_model.sav')\n",
    "    nb_est = joblib.load('./NB_tuned_model.sav')\n",
    "    return (lr_est.best_estimator_.predict([s])[0], svc_est.best_estimator_.predict([s])[0], nb_est.best_estimator_.predict([s])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_df = pd.read_csv(filepath_or_buffer='../input/load-summaries/summaries_for_testing.csv', header=0,names=['title','author','genre','summary'] )\n",
    "goodreads_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= { 'Title': [],\n",
    "           'Author': [],\n",
    "           'Prediction LR': [],\n",
    "           'Prediction SVC': [],\n",
    "           'Prediction NB': [],\n",
    "           'Genre': []\n",
    "                }\n",
    "plots = goodreads_df.summary\n",
    "titles = goodreads_df.title\n",
    "authors = goodreads_df.author\n",
    "genres = goodreads_df.genre\n",
    "for i in range(len(goodreads_df)):\n",
    "    (lr, svc, nb) = predict_genre(plot=plots[i])\n",
    "    results['Title'].append(titles[i])\n",
    "    results['Author'].append(authors[i])\n",
    "    results['Prediction LR'].append(lr)\n",
    "    results['Prediction SVC'].append(svc)\n",
    "    results['Prediction NB'].append(nb)\n",
    "    results['Genre'].append(genres[i])\n",
    "results_df = pd.DataFrame.from_dict(results, orient='columns')\n",
    "results_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
