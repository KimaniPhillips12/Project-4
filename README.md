# Project-4

# Introduction
Our objective for this project was to train and evaluate various machine learning models to determine which deep learning model could effectively learn and predict a book's genre based on its plot summary.

# Models

BERT (Bidirectional Encoder Representations from Transformers): BERT is a type of deep learning model that is specifically designed for natural language processing (NLP) tasks. It uses a Transformer architecture, which allows it to understand the context of words in a sentence by considering both the left and right context simultaneously.

LSTM (Long Short-Term Memory): LSTM is a type of recurrent neural network (RNN) architecture that is designed to capture long-term dependencies in sequential data. It has a memory cell that can store information over time, allowing it to learn patterns in sequential data. LSTM is commonly used for sequential data tasks such as time series forecasting, speech recognition, language translation, and text generation.

SVC (Support Vector Classifier): SVC is a type of supervised learning algorithm that is used for classification tasks. It works by finding the hyperplane that best separates different classes in the feature space. SVC is a powerful classification algorithm that is widely used in both binary and multi-class classification problems. SVC is effective in high-dimensional spaces and is robust to overfitting when the number of features is greater than the number of samples.

Logistic Regression: Logistic regression is a type of linear regression model that is used for binary classification tasks. It models the probability that a given input belongs to a particular class using a logistic function.

Naive Bayes: Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem with an assumption of independence between features. It calculates the probability of each class given a set of input features and selects the class with the highest probability. Naive Bayes is simple, fast, and requires a small amount of training data. It can handle a large number of features and is robust to irrelevant features. Naive Bayes is commonly used in text classification tasks such as spam filtering, sentiment analysis, and document categorization.

XGBoost (Extreme Gradient Boosting): XGBoost is a type of gradient boosting algorithm that builds an ensemble of weak learners (decision trees) sequentially, where each tree corrects the errors of its predecessor. It uses a gradient descent optimization technique to minimize a loss function.

